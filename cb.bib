@article{NeuroBayes,
title = "The NeuroBayes neural network package ",
author    = "M. Feindt and U. Kerzel",
journal = "NIM A",
volume = "559",
number = "1",
pages = "190 - 194",
year = "2006",
url = "http://www.sciencedirect.com/science/article/pii/S0168900205022679",
keywords = "Bayes",
keywords = "Neural network",
keywords = "Classification",
keywords = "Density reconstruction",
keywords = "Data-mining",
keywords = "Preprocessing "
}

@inproceedings{lime,
  author    = {Marco Tulio Ribeiro and
               Sameer Singh and
               Carlos Guestrin},
  title     = {"Why Should {I} Trust You?": Explaining the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} International Conference on
               Knowledge Discovery and Data Mining, San Francisco, CA, USA, August
               13-17, 2016},
  pages     = {1135--1144},
  year      = {2016},
}

@article{SHAP,
  author    = {Scott Lundberg and
               Su{-}In Lee},
  title     = {A unified approach to interpreting model predictions},
  journal   = {CoRR},
  volume    = {abs/1705.07874},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.07874},
  archivePrefix = {arXiv},
  eprint    = {1705.07874},
  timestamp = {Wed, 07 Jun 2017 14:42:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/LundbergL17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Shapeley1953,
author={L. S. Shapley},
title={A Value for n-Person Games},
journal={Cntributions to the Theory of Games II, Annals of Mathematics Studies},
volume={28},
year={1953},
pages={307-317}
}

@Article{Edgeworth,
  author = 	 {F.Y. Edgeworth},
  title = 	 {The Mathematical Theory of Banking},
  journal = 	 {Journal of the Royal Statistical Society},
  year = 	 {1888},
  OPTkey = 	 {},
  OPTvolume = 	 {53},
  OPTnumber = 	 {},
  OPTpages = 	 {113-127},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Article{Scarf1959,
  author = 	 {Herbert Scarf},
  title = 	 {The Optimality of (S,s) Policies in the Dynamic Inventory Problem},
  journal = 	 {Mathematical Methods in the Social Sciences},
  year = 	 {1959},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@article {Nahmias1973,
author = {Nahmias, Steven and Pierskalla, William P.},
title = {Optimal ordering policies for a product that perishes in two periods subject to stochastic demand},
journal = {Naval Research Logistics Quarterly},
volume = {20},
number = {2},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
issn = {1931-9193},
url = {http://dx.doi.org/10.1002/nav.3800200202},
doi = {10.1002/nav.3800200202},
pages = {207--229},
year = {1973},
}

@article{Khouja1999537,
title = "The single-period (news-vendor) problem: literature review and suggestions for future research ",
journal = "Omega ",
volume = "27",
number = "5",
pages = "537 - 553",
year = "1999",
note = "",
issn = "0305-0483",
doi = "http://dx.doi.org/10.1016/S0305-0483(99)00017-1",
url = "http://www.sciencedirect.com/science/article/pii/S0305048399000171",
author = "Moutaz Khouja",
keywords = "Inventory management and production/operations management "
}

@article{Ehrenberg1959,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2985810},
 author = {A. S. C. Ehrenberg},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {1},
 pages = {26--41},
 publisher = {[Wiley, Royal Statistical Society]},
 title = {The Pattern of Consumer Purchases},
 volume = {8},
 year = {1959}
}

@book{GAM,
  author    = {T.J. Hastie and R.J. Tibshirani},
  title     = {Generalized Additive Models},
  publisher = {Chapman and Hall/CRC},
  year      = 1990,
  isbn      = {978-0-412-34390-2}
}

@article{KELLEYPACE1997291,
title = "Sparse spatial autoregressions",
journal = "Statistics \& Probability Letters",
volume = "33",
number = "3",
pages = "291 - 297",
year = "1997",
issn = "0167-7152",
doi = "https://doi.org/10.1016/S0167-7152(96)00140-X",
url = "http://www.sciencedirect.com/science/article/pii/S016771529600140X",
author = "R. Kelley Pace and Ronald Barry",
keywords = "Spatial autoregression, SAR, Sparse matrices"
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@misc{GBR,
  author = {scikit-learn},
  howpublished = {\url{http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html}},
  note = {Version: 0.19.1}
}

@misc{kaggle_data,
  howpublished = {\url{https://www.kaggle.com/c/demand-forecasting-kernels-only/data}}
}

@misc{kaggle_results,
  howpublished = {\url{https://www.kaggle.com/c/demand-forecasting-kernels-only/leaderboard}}
}

@Article{Wright2015,
author="Wright, Stephen J.",
title="Coordinate descent algorithms",
journal="Mathematical Programming",
year="2015",
month="Jun",
day="01",
volume="151",
number="1",
pages="3--34",
abstract="Coordinate descent algorithms solve optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes. They have been used in applications for many years, and their popularity continues to grow because of their usefulness in data analysis, machine learning, and other areas of current interest. This paper describes the fundamentals of the coordinate descent approach, together with variants and extensions and their convergence properties, mostly with reference to convex objectives. We pay particular attention to a certain problem structure that arises frequently in machine learning applications, showing that efficient implementations of accelerated coordinate descent algorithms are possible for problems of this type. We also present some parallel variants and discuss their convergence properties under several models of parallel execution.",
issn="1436-4646",
doi="10.1007/s10107-015-0892-3",
url="https://doi.org/10.1007/s10107-015-0892-3"
}

@book{molnar2019,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  note       = {\url{https://christophm.github.io/interpretable-ml-book/}},
  year       = {2019},
  subtitle   = {A Guide for Making Black Box Models Explainable}
}

@article{yeh2019,
title={Representer Point Selection for Explainnig Deep Neural Networks},
author={C.K. Yeh and J.S. Kim and I.E.H. Yen and P. Ramikumar},
journal = {NeurIPS 2018},
volume    = {abs/1811.09720},
 year      = {2018},
  url       = {http://arxiv.org/abs/1811.09720},
 archivePrefix = {arXiv},
 eprint    = {1811.09720}
}

@misc{Gee2019,
Author = {Alan H. Gee and Diego Garcia-Olano and Joydeep Ghosh and David Paydarfar},
Title = {Explaining Deep Classification of Time-Series Data with Learned Prototypes},
Year = {2019},
Eprint = {arXiv:1904.08935},
}

@article{olah2017feature,
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  title = {Feature Visualization},
  journal = {Distill},
  year = {2017},
  note = {https://distill.pub/2017/feature-visualization},
  doi = {10.23915/distill.00007}
}

@misc{GoogleWhatIf,
title={The What-If Tool: Code-Free Probing of Machine Learning Models},
author={Google},
howpublished={\url{https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/interactive_inference}}
}

@article{Marscharka2018,
Author = {David Mascharka and Philip Tran and Ryan Soklaski and Arjun Majumdar},
Title = {Transparency by Design: Closing the Gap Between Performance and Interpretability in Visual Reasoning},
Year = {2018},
Eprint = {arXiv:1803.05268},
Doi = {10.1109/CVPR.2018.00519}
}

@article{Henderson2017,
Author = {Ryan Henderson and Rasmus Rothe},
Title = {Picasso: A Modular Framework for Visualizing the Learning Process of Neural Network Image Classifiers},
Year = {2017},
Eprint = {arXiv:1705.05627},
Howpublished = {Journal of Open Research Software. 5(1), p.22 (2017)},
Doi = {10.5334/jors.178},
}

@misc{Yellowbrick,
author={District Data Labs},
title={Yellowbrick: Machine Learning Visualization},
howpublished={\url{http://www.scikit-yb.org}}
}

@misc{eli5,
author={TeamHG-Memex},
title={eli5: A library for debugging/inspecting machine learning classifiers and explaining their predictions},
howpublished={\url{https://github.com/TeamHG-Memex/eli5}}
}

@misc{Strobelt2016,
Author = {Hendrik Strobelt and Sebastian Gehrmann and Hanspeter Pfister and Alexander M. Rush},
Title = {LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks},
Year = {2016},
Eprint = {arXiv:1606.07461},
}

@Article{Breiman2001,
author="Breiman, Leo",
title="Random Forests",
journal="Machine Learning",
year="2001",
month="Oct",
day="01",
volume="45",
number="1",
pages="5--32",
abstract="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
issn="1573-0565",
doi="10.1023/A:1010933404324",
url="https://doi.org/10.1023/A:1010933404324"
}

@article{friedman2001,
author = "Friedman, Jerome H.",
doi = "10.1214/aos/1013203451",
fjournal = "The Annals of Statistics",
journal = "Ann. Statist.",
month = "10",
number = "5",
pages = "1189--1232",
publisher = "The Institute of Mathematical Statistics",
title = "Greedy function approximation: A gradient boosting machine.",
url = "https://doi.org/10.1214/aos/1013203451",
volume = "29",
year = "2001"
}

@misc{Simonyan2013,
Author = {Karen Simonyan and Andrea Vedaldi and Andrew Zisserman},
Title = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
Year = {2013},
Eprint = {arXiv:1312.6034}
}

@misc{Zeiler2013,
Author = {Matthew D Zeiler and Rob Fergus},
Title = {Visualizing and Understanding Convolutional Networks},
Year = {2013},
Eprint = {arXiv:1311.2901}
}

@article{GLM,
 ISSN = {00359238},
 URL = {http://www.jstor.org/stable/2344614},
 abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
 author = {J. A. Nelder and R. W. M. Wedderburn},
 journal = {Journal of the Royal Statistical Society. Series A (General)},
 number = {3},
 pages = {370--384},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Generalized Linear Models},
 volume = {135},
 year = {1972}
}

@misc{Interprete,
author={Microsoft},
title={InterpretML},
howpublished={\url{https://github.com/microsoft/interpret}}
}

@inproceedings{GAM_Microsoft,
 author = {Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},
 title = {Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission},
 booktitle = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 series = {KDD '15},
 year = {2015},
 isbn = {978-1-4503-3664-2},
 location = {Sydney, NSW, Australia},
 pages = {1721--1730},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2783258.2788613},
 doi = {10.1145/2783258.2788613},
 acmid = {2788613},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {additive models, classification, healthcare, intelligibility, interaction detection, logistic regression, risk prediction},
} 

@inproceedings{Boser1992,
  doi = {10.1145/130385.130401},
  url = {https://doi.org/10.1145/130385.130401},
  year = {1992},
  publisher = {{ACM} Press},
  author = {Bernhard E. Boser and Isabelle M. Guyon and Vladimir N. Vapnik},
  title = {A training algorithm for optimal margin classifiers},
  booktitle = {Proceedings of the fifth annual workshop on Computational learning theory  - {COLT} {\textquotesingle}92}
}
